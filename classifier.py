"""
	Classifier - CNN
	Classify images from different families with a CNN for the private Cisco dataset

	Author: Benoit Michel
	Date : June 2021
"""

#from numpy.random import seed
#seed(1)
#from tensorflow import set_random_seed
#set_random_seed(2)

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import keras
import seaborn as sns
from keras.models import Sequential, Input, Model
from keras import to_categorical
from keras.layers import Dense, Dropout, Flatten, Softmax
from keras.layers import Conv2D, MaxPooling2D
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight, shuffle
from sklearn import metrics
from PIL import Image
import time
import argparse
import os
import shutil
from PIL import ImageFile
from Utils.utils_cnn import print_GridCV_scores,print_score,My_Custom_Generator
from datetime import datetime
print("hey")
ImageFile.LOAD_TRUNCATED_IMAGES = True

Image.MAX_IMAGE_PIXELS = None # To avoid warnings with big images

""" Arguments management """
parser = argparse.ArgumentParser(prog='classifier.py', description="Convert binary file to image")
parser.add_argument("input", help="Input directory path for images. This directory should contain several directories with names related to labels of images")
parser.add_argument("--save", help="Save model trained")
parser.add_argument("--generate", help="Generate dataset for cross validation in this directory", default=None)
parser.add_argument("--grid_search", help="Launch grid search to find optimal hyperparameters", action='store_true')
parser.add_argument("--out", help="Filename used to store results (default timestamp.csv )", default=datetime.now().strftime("%m-%d-%Y-%H:%M:%S")+'.csv')

args = parser.parse_args()

if args.generate:
    train_dir = args.input
    dest_dir = args.generate
    if not os.path.exists(args.generate):
        os.mkdir(args.generate)
    counter = 0

    for subdir, dirs, files in os.walk(train_dir):
        for file in files:
            full_path = os.path.join(subdir, file)
            shutil.copy(full_path, dest_dir)
            counter = counter + 1  
    filenames = []
    labels = np.zeros((counter, 1))

    filenames_counter = 0
    labels_counter = -1

    for subdir, dirs, files in os.walk(train_dir):
        for file in files:
            filenames.append(file)
            labels[filenames_counter, 0] = labels_counter
            filenames_counter = filenames_counter + 1
        labels_counter = labels_counter+1
        
    np.save(args.generate+'/filenames.npy', filenames)

    # One hot vector representation of labels
    y_labels_one_hot = to_categorical(labels)

    # saving the y_labels_one_hot array as a .npy file
    np.save(args.generate+'/y_labels_one_hot.npy', y_labels_one_hot)
    filenames_shuffled, y_labels_one_hot_shuffled = shuffle(filenames, y_labels_one_hot)

    # saving the shuffled file.
    # you can load them later using np.load().
    np.save(args.generate+'/y_labels_one_hot_shuffled.npy', y_labels_one_hot_shuffled)
    np.save(args.generate+'/filenames_shuffled.npy', filenames_shuffled)
    
    filenames_shuffled_numpy = np.array(filenames_shuffled)

    X_train_filenames, X_val_filenames, y_train, y_val = train_test_split(
        filenames_shuffled_numpy, y_labels_one_hot_shuffled, test_size=0.2, random_state=1)

    # You can save these files as well. As you will be using them later for training and validation of your model.
    np.save('X_train_filenames.npy', X_train_filenames)
    np.save('y_train.npy', y_train)

    np.save('X_val_filenames.npy', X_val_filenames)
    np.save('y_val.npy', y_val)
    dir_data = args.generate
else:
    X_train_filenames = np.load(args.input+'X_train_filenames.npy')
    y_train = np.load(args.input+'y_train.npy')
    X_val_filenames = np.load(args.input+'X_val_filenames.npy')
    y_val = np.load(args.input+'y_val.npy')
    dir_data = args.input

batch_size = 32

my_training_batch_generator = My_Custom_Generator(dir_data,(224,224,3),X_train_filenames, y_train, batch_size)
my_validation_batch_generator = My_Custom_Generator(dir_data,(224,224,3),X_val_filenames, y_val, batch_size)
classes = y_train.shape[1]


#batches = ImageDataGenerator().flow_from_directory(directory=args.input, target_size=(224,224), batch_size=10000) # TODO : DISCUSS THIS TARGET SIZE !


#imgs, labels = next(batches)

#classes = batches.class_indices.keys()
#perc = (sum(labels)/labels.shape[0])*100



def malware_model(optimizer='adam',learn_rate=0.1,activation='relu',loss='categorical_crossentropy',dropout1=0.25,dropout2=0.5,neurons1=1024,neurons2=512):
    if optimizer == 'SGD':
        opt = keras.optimizers.SGD(lr=learn_rate, nesterov=True)
    else :   
        opt = keras.optimizers.Adam(learning_rate=learn_rate)
    Malware_model = Sequential()
    Malware_model.add(Conv2D(30, kernel_size=(3, 3),
                     activation=activation,
                     input_shape=(224,224,3)))
    Malware_model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
    Malware_model.add(Conv2D(15, (3, 3), activation=activation))
    Malware_model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
    Malware_model.add(Dropout(dropout1))
    Malware_model.add(Flatten())
    Malware_model.add(Dense(neurons1, activation=activation))
    Malware_model.add(Dropout(dropout2))
    Malware_model.add(Dense(neurons2, activation=activation))
    #Malware_model.add(Dropout(j))
    Malware_model.add(Dense(num_classes, activation='softmax'))
    Malware_model.compile(loss=loss, optimizer = opt, metrics=['accuracy'])
    return Malware_model


if args.grid_search:
    """ GRID SEARCH CONFIGURATION """
    print('Let\'s start grid search !')
    # define the grid search parameters
    model = KerasClassifier(build_fn=malware_model, verbose=0)
    optimizer = ['SGD','adam']
    learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]
    batch_size = [10, 20, 40, 60, 80, 100]
    dropout = [0.25,0.5,0.75]
    neurons = [128,256,512,1024,2048,4096]
    activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'linear']
    loss = ['categorical_crossentropy','kullback_leibler_divergence','log_cosh','mean_squared_error']
    param_grid = dict(learn_rate=learn_rate, optimizer=optimizer,batch_size=batch_size,activation=activation,loss=loss,dropout1=dropout,dropout2=dropout,neurons1=neurons,neurons2=neurons)

    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)
    grid_result = grid.fit(X, Y)


    print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
    means = grid_result.cv_results_['mean_test_score']
    stds = grid_result.cv_results_['std_test_score']
    params = grid_result.cv_results_['params']
    for mean, stdev, param in zip(means, stds, params):
        print("%f (%f) with: %r" % (mean, stdev, param))
    print_GridCV_scores(grid,args.out)
    exit(0)



accuracies = list()

#X_train, X_test, y_train, y_test = train_test_split(imgs/255.,labels, test_size=0.3) # Divided by 255 to have feature btw 0 and 1

Malware_model = malware_model()
Malware_model.summary()


es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)
print('Start training of the model !')
Malware_model.fit_generator(generator=my_training_batch_generator,steps_per_epoch= int(y_train.shape[0]//batch_size), validation_data=my_validation_batch_generator,validation_steps=steps_per_epoch= int(y_val.shape[0]//batch_size) epochs=20, verbose=1, callbacks=[es])
scores = Malware_model.evaluate(X_test, y_test)


print('Final CNN accuracy: ', scores[1])
accuracies.append(scores[1])

if args.save:
    Malware_model.save(args.save)
    
print_score(Malware_model,scores,args.out)


"""
# Confusion matrix
y_pred = Malware_model.predict_classes(X_test, verbose=0)
y_test2 = np.argmax(y_test, axis=1)
c_matrix = metrics.confusion_matrix(y_test2, y_pred)

def confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):

    df_cm = pd.DataFrame(
        confusion_matrix, index=class_names, columns=class_names,
    )
    fig = plt.figure(figsize=figsize)
    try:
        heatmap = sns.heatmap(df_cm, annot=True, fmt="d")
    except ValueError:
        raise ValueError("Confusion matrix values must be integers.")
    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)
    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

class_names= batches.class_indices.keys()
confusion_matrix(c_matrix, class_names, figsize = (20,7), fontsize=14)"""
